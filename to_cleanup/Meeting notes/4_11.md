#4.11

##Definition of ùõº
$$k_c = Number\ of\ instances\ with\ class\ label\ c - 1 $$
For each instance $x_i$:  
$$\alpha = \frac{Number\ of\ k\ nearest\ instances\ with\ class\ label\ x_i.class}{k_{x_i.class}}$$  

We assume that instances of the same class are closer to each other in the meta feature space. $\alpha$ measures how well instances of the same class are grouped together.  
$\alpha$ also describes the complexity by the decision boundary as instances with higher $\alpha$ value are grouped with more instances of the same class, which means smaller decision boundary and lower complexity. 

##Problem  
We define a dataset $X$ with $n$ instances, where $X=\{x_i\},\ i\in \{1,2,...,n\}$. For each $x_i$, a class label $y_i$ is assigned, where the set of all class labels, $Y=\{y_i\},\ i\in \{1,2,...,n\}$. For each $x_i$, we can calculate $\alpha$ for $(X,Y)$ to get a  $\alpha_{i}$, where $i\in \{1,2,...,n\}$.

We define a function $f(X,Y)$ which output the possibility of each instances being classified as each of the class labels. The new metrix formed, $X'$, from the output of $f$ is a mapping of the initial dataset. For each $x'_i$, where $x'_i \in X'$, we can calculate $\alpha$ for $(X',Y)$ to get a $\alpha'_{i}$, where $i\in \{1,2,...,n\}$. 

How can we convert the two lists, $\alpha$ and $\alpha'$to meta feature(s) for a meta learning problem? 